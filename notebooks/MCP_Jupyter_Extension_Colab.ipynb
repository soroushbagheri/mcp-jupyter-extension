{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP-Jupyter Extension: Multi-LLM Colab Notebook\n",
    "\n",
    "This notebook demonstrates the complete MCP-Jupyter Extension with support for multiple LLMs.\n",
    "\n",
    "**Supported Providers:**\n",
    "- OpenAI (GPT-4, GPT-3.5-Turbo)\n",
    "- Anthropic (Claude 3 family)\n",
    "- Google (Gemini Pro)\n",
    "- Hugging Face (Any HF model)\n",
    "- Cohere (Command models)\n",
    "- Local Models (Ollama, vLLM)\n",
    "\n",
    "**Created:** February 2026\n",
    "**Version:** 1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup and Installation\n",
    "\n",
    "Run this cell first to install all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install --upgrade pip\n",
    "!pip install -q openai anthropic google-generativeai huggingface-hub cohere python-dotenv aiohttp pydantic requests\n",
    "\n",
    "# Verify installations\n",
    "import sys\n",
    "print(f'Python version: {sys.version}')\n",
    "print('All packages installed successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Import Libraries and Configure Credentials\n",
    "\n",
    "Configure your API keys for different LLM providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Optional, Dict, List, Iterator\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# Import LLM libraries\n",
    "import openai\n",
    "from anthropic import Anthropic\n",
    "import google.generativeai as genai\n",
    "from huggingface_hub import InferenceClient\n",
    "import cohere\n",
    "\n",
    "print('âœ… All libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure API Keys\n",
    "\n",
    "Enter your API keys below. You can get them from:\n",
    "- [OpenAI](https://platform.openai.com/api-keys)\n",
    "- [Anthropic](https://console.anthropic.com/)\n",
    "- [Google AI](https://ai.google.dev/)\n",
    "- [Hugging Face](https://huggingface.co/settings/tokens)\n",
    "- [Cohere](https://dashboard.cohere.ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from getpass import getpass\n",
    "\n",
    "# Configure API keys (use getpass for security)\n",
    "api_keys = {}\n",
    "\n",
    "# OpenAI\n",
    "openai_key = getpass('OpenAI API Key (or press Enter to skip): ')\n",
    "if openai_key:\n",
    "    os.environ['OPENAI_API_KEY'] = openai_key\n",
    "    api_keys['openai'] = openai_key\n",
    "\n",
    "# Anthropic\n",
    "anthropic_key = getpass('Anthropic API Key (or press Enter to skip): ')\n",
    "if anthropic_key:\n",
    "    os.environ['ANTHROPIC_API_KEY'] = anthropic_key\n",
    "    api_keys['anthropic'] = anthropic_key\n",
    "\n",
    "# Google Gemini\n",
    "google_key = getpass('Google API Key (or press Enter to skip): ')\n",
    "if google_key:\n",
    "    os.environ['GOOGLE_API_KEY'] = google_key\n",
    "    api_keys['google'] = google_key\n",
    "    genai.configure(api_key=google_key)\n",
    "\n",
    "# Hugging Face\n",
    "hf_key = getpass('Hugging Face API Key (or press Enter to skip): ')\n",
    "if hf_key:\n",
    "    os.environ['HF_API_KEY'] = hf_key\n",
    "    api_keys['huggingface'] = hf_key\n",
    "\n",
    "# Cohere\n",
    "cohere_key = getpass('Cohere API Key (or press Enter to skip): ')\n",
    "if cohere_key:\n",
    "    os.environ['COHERE_API_KEY'] = cohere_key\n",
    "    api_keys['cohere'] = cohere_key\n",
    "\n",
    "print(f'\\nâœ… Configured {len(api_keys)} API providers')\n",
    "print(f'Available providers: {list(api_keys.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Define MCP Client Classes\n",
    "\n",
    "Core classes for managing LLM interactions through the MCP protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@dataclass\n",
    "class MCPResponse:\n",
    "    \"\"\"Response object from LLM\"\"\"\n",
    "    content: str\n",
    "    provider: str\n",
    "    model: str\n",
    "    timestamp: str\n",
    "    tokens_used: Optional[int] = None\n",
    "    finish_reason: Optional[str] = None\n",
    "\n",
    "\n",
    "class BaseLLMProvider(ABC):\n",
    "    \"\"\"Base class for LLM providers\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "    \n",
    "    @abstractmethod\n",
    "    def query(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> MCPResponse:\n",
    "        \"\"\"Send a query to the LLM\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def stream(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> Iterator[str]:\n",
    "        \"\"\"Stream response from the LLM\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class OpenAIProvider(BaseLLMProvider):\n",
    "    \"\"\"OpenAI GPT provider\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"gpt-4\"):\n",
    "        super().__init__(api_key, model)\n",
    "        self.client = openai.OpenAI(api_key=api_key)\n",
    "    \n",
    "    def query(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> MCPResponse:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return MCPResponse(\n",
    "            content=response.choices[0].message.content,\n",
    "            provider=\"OpenAI\",\n",
    "            model=self.model,\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            tokens_used=response.usage.total_tokens,\n",
    "            finish_reason=response.choices[0].finish_reason\n",
    "        )\n",
    "    \n",
    "    def stream(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> Iterator[str]:\n",
    "        stream = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            stream=True\n",
    "        )\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content:\n",
    "                yield chunk.choices[0].delta.content\n",
    "\n",
    "\n",
    "class AnthropicProvider(BaseLLMProvider):\n",
    "    \"\"\"Anthropic Claude provider\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"claude-3-sonnet-20240229\"):\n",
    "        super().__init__(api_key, model)\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "    \n",
    "    def query(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> MCPResponse:\n",
    "        response = self.client.messages.create(\n",
    "            model=self.model,\n",
    "            max_tokens=max_tokens,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return MCPResponse(\n",
    "            content=response.content[0].text,\n",
    "            provider=\"Anthropic\",\n",
    "            model=self.model,\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            tokens_used=response.usage.output_tokens + response.usage.input_tokens,\n",
    "            finish_reason=response.stop_reason\n",
    "        )\n",
    "    \n",
    "    def stream(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> Iterator[str]:\n",
    "        with self.client.messages.stream(\n",
    "            model=self.model,\n",
    "            max_tokens=max_tokens,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature\n",
    "        ) as stream:\n",
    "            for text in stream.text_stream:\n",
    "                yield text\n",
    "\n",
    "\n",
    "class GoogleProvider(BaseLLMProvider):\n",
    "    \"\"\"Google Gemini provider\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"gemini-pro\"):\n",
    "        super().__init__(api_key, model)\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.client = genai.GenerativeModel(model_name=model)\n",
    "    \n",
    "    def query(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> MCPResponse:\n",
    "        response = self.client.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                max_output_tokens=max_tokens,\n",
    "                temperature=temperature\n",
    "            )\n",
    "        )\n",
    "        return MCPResponse(\n",
    "            content=response.text,\n",
    "            provider=\"Google\",\n",
    "            model=self.model,\n",
    "            timestamp=datetime.now().isoformat()\n",
    "        )\n",
    "    \n",
    "    def stream(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> Iterator[str]:\n",
    "        response = self.client.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                max_output_tokens=max_tokens,\n",
    "                temperature=temperature\n",
    "            ),\n",
    "            stream=True\n",
    "        )\n",
    "        for chunk in response:\n",
    "            if chunk.text:\n",
    "                yield chunk.text\n",
    "\n",
    "\n",
    "class HuggingFaceProvider(BaseLLMProvider):\n",
    "    \"\"\"Hugging Face Inference API provider\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"mistralai/Mistral-7B-Instruct-v0.1\"):\n",
    "        super().__init__(api_key, model)\n",
    "        self.client = InferenceClient(api_key=api_key)\n",
    "    \n",
    "    def query(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> MCPResponse:\n",
    "        response = self.client.text_generation(\n",
    "            prompt=prompt,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return MCPResponse(\n",
    "            content=response,\n",
    "            provider=\"HuggingFace\",\n",
    "            model=self.model,\n",
    "            timestamp=datetime.now().isoformat()\n",
    "        )\n",
    "    \n",
    "    def stream(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> Iterator[str]:\n",
    "        for token in self.client.text_generation(\n",
    "            prompt=prompt,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            stream=True\n",
    "        ):\n",
    "            yield token\n",
    "\n",
    "\n",
    "class CohereProvider(BaseLLMProvider):\n",
    "    \"\"\"Cohere API provider\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"command\"):\n",
    "        super().__init__(api_key, model)\n",
    "        self.client = cohere.Client(api_key=api_key)\n",
    "    \n",
    "    def query(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> MCPResponse:\n",
    "        response = self.client.generate(\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return MCPResponse(\n",
    "            content=response.generations[0].text,\n",
    "            provider=\"Cohere\",\n",
    "            model=self.model,\n",
    "            timestamp=datetime.now().isoformat()\n",
    "        )\n",
    "    \n",
    "    def stream(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> Iterator[str]:\n",
    "        # Note: Cohere streaming not directly supported, fall back to regular generation\n",
    "        response = self.query(prompt, temperature, max_tokens)\n",
    "        yield response.content\n",
    "\n",
    "\n",
    "print('âœ… Provider classes defined successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Define MCPClient for Easy Access\n",
    "\n",
    "High-level client to switch between providers seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MCPClient:\n",
    "    \"\"\"High-level client for MCP-Jupyter Extension\"\"\"\n",
    "    \n",
    "    PROVIDERS = {\n",
    "        'openai': OpenAIProvider,\n",
    "        'anthropic': AnthropicProvider,\n",
    "        'google': GoogleProvider,\n",
    "        'huggingface': HuggingFaceProvider,\n",
    "        'cohere': CohereProvider,\n",
    "    }\n",
    "    \n",
    "    def __init__(self, provider: str, model: str = None):\n",
    "        if provider not in self.PROVIDERS:\n",
    "            raise ValueError(f\"Provider '{provider}' not supported. Choose from {list(self.PROVIDERS.keys())}\")\n",
    "        \n",
    "        self.provider_name = provider\n",
    "        api_key = os.environ.get(f\"{provider.upper()}_API_KEY\".replace('OPENAI_API_KEY', 'OPENAI_API_KEY').replace('ANTHROPIC', 'ANTHROPIC').replace('GOOGLE', 'GOOGLE').replace('HUGGINGFACE', 'HF').replace('COHERE', 'COHERE'))\n",
    "        \n",
    "        if provider == 'openai':\n",
    "            api_key = os.environ.get('OPENAI_API_KEY')\n",
    "        elif provider == 'anthropic':\n",
    "            api_key = os.environ.get('ANTHROPIC_API_KEY')\n",
    "        elif provider == 'google':\n",
    "            api_key = os.environ.get('GOOGLE_API_KEY')\n",
    "        elif provider == 'huggingface':\n",
    "            api_key = os.environ.get('HF_API_KEY')\n",
    "        elif provider == 'cohere':\n",
    "            api_key = os.environ.get('COHERE_API_KEY')\n",
    "        \n",
    "        if not api_key:\n",
    "            raise ValueError(f\"API key not found for provider '{provider}'.\")\n",
    "        \n",
    "        # Default models\n",
    "        default_models = {\n",
    "            'openai': 'gpt-4',\n",
    "            'anthropic': 'claude-3-sonnet-20240229',\n",
    "            'google': 'gemini-pro',\n",
    "            'huggingface': 'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "            'cohere': 'command'\n",
    "        }\n",
    "        \n",
    "        model = model or default_models[provider]\n",
    "        self.provider = self.PROVIDERS[provider](api_key, model)\n",
    "    \n",
    "    def query(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> MCPResponse:\n",
    "        \"\"\"Send a query and get response\"\"\"\n",
    "        try:\n",
    "            return self.provider.query(prompt, temperature, max_tokens)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error with {self.provider_name}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def stream(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000):\n",
    "        \"\"\"Stream response from LLM\"\"\"\n",
    "        try:\n",
    "            for chunk in self.provider.stream(prompt, temperature, max_tokens):\n",
    "                print(chunk, end='', flush=True)\n",
    "                yield chunk\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Error streaming from {self.provider_name}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "print('âœ… MCPClient defined successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Example 1 - Single Provider Query\n",
    "\n",
    "Test with a single LLM provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example query\n",
    "test_prompt = \"Explain quantum computing in 3 sentences.\"\n",
    "\n",
    "# Choose a provider that you configured\n",
    "available_providers = list(api_keys.keys())\n",
    "\n",
    "if available_providers:\n",
    "    provider = available_providers[0]\n",
    "    print(f\"ðŸš€ Testing with {provider}...\\n\")\n",
    "    \n",
    "    try:\n",
    "        client = MCPClient(provider=provider)\n",
    "        response = client.query(test_prompt)\n",
    "        \n",
    "        print(f\"Provider: {response.provider}\")\n",
    "        print(f\"Model: {response.model}\")\n",
    "        print(f\"Response:\\n{response.content}\")\n",
    "        print(f\"\\nTimestamp: {response.timestamp}\")\n",
    "        if response.tokens_used:\n",
    "            print(f\"Tokens used: {response.tokens_used}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"No API keys configured. Please run Part 2 to add API keys.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Example 2 - Streaming Response\n",
    "\n",
    "Get real-time streamed responses from LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Streaming example\n",
    "stream_prompt = \"Write a creative haiku about artificial intelligence.\"\n",
    "\n",
    "if available_providers:\n",
    "    provider = available_providers[0]\n",
    "    print(f\"ðŸŒŠ Streaming from {provider}...\\n\")\n",
    "    \n",
    "    try:\n",
    "        client = MCPClient(provider=provider)\n",
    "        print(\"Response: \")\n",
    "        for _ in client.stream(stream_prompt):\n",
    "            pass\n",
    "        print(\"\\n\\nâœ… Streaming complete!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Example 3 - Multi-Provider Comparison\n",
    "\n",
    "Compare responses from multiple LLM providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Multi-provider comparison\n",
    "comparison_prompt = \"What are the top 3 applications of AI in healthcare?\"\n",
    "\n",
    "results = []\n",
    "\n",
    "for provider in available_providers:\n",
    "    print(f\"ðŸ“ Querying {provider}...\")\n",
    "    try:\n",
    "        client = MCPClient(provider=provider)\n",
    "        response = client.query(comparison_prompt, max_tokens=300)\n",
    "        results.append({\n",
    "            'Provider': response.provider,\n",
    "            'Model': response.model,\n",
    "            'Response': response.content[:200] + '...' if len(response.content) > 200 else response.content,\n",
    "            'Timestamp': response.timestamp\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error with {provider}: {e}\")\n",
    "\n",
    "if results:\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\nðŸ“Š Comparison Results:\")\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"No results to compare.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Context Management\n",
    "\n",
    "Manage conversation context and system instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ContextManager:\n",
    "    \"\"\"Manages conversation context and system instructions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.context = []\n",
    "        self.system_instruction = \"You are a helpful AI assistant.\"\n",
    "    \n",
    "    def add_message(self, role: str, content: str):\n",
    "        \"\"\"Add a message to context\"\"\"\n",
    "        self.context.append({\"role\": role, \"content\": content})\n",
    "    \n",
    "    def add_system_instruction(self, instruction: str):\n",
    "        \"\"\"Set system instruction\"\"\"\n",
    "        self.system_instruction = instruction\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear all context\"\"\"\n",
    "        self.context = []\n",
    "    \n",
    "    def get_context(self) -> Dict:\n",
    "        \"\"\"Get full context\"\"\"\n",
    "        return {\n",
    "            \"system\": self.system_instruction,\n",
    "            \"messages\": self.context\n",
    "        }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "ctx = ContextManager()\n",
    "ctx.add_system_instruction(\"You are an expert Python programmer.\")\n",
    "ctx.add_message(\"user\", \"What's the best way to handle errors in Python?\")\n",
    "\n",
    "print(\"Context:\")\n",
    "print(json.dumps(ctx.get_context(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Advanced Configuration\n",
    "\n",
    "Configure provider-specific settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configuration dictionary\n",
    "provider_config = {\n",
    "    'openai': {\n",
    "        'models': ['gpt-4', 'gpt-4-turbo-preview', 'gpt-3.5-turbo'],\n",
    "        'temperature_range': (0, 2),\n",
    "        'max_tokens': 4096\n",
    "    },\n",
    "    'anthropic': {\n",
    "        'models': ['claude-3-opus-20240229', 'claude-3-sonnet-20240229', 'claude-3-haiku-20240307'],\n",
    "        'temperature_range': (0, 1),\n",
    "        'max_tokens': 4096\n",
    "    },\n",
    "    'google': {\n",
    "        'models': ['gemini-pro', 'gemini-pro-vision'],\n",
    "        'temperature_range': (0, 2),\n",
    "        'max_tokens': 32000\n",
    "    },\n",
    "    'huggingface': {\n",
    "        'models': ['mistralai/Mistral-7B-Instruct-v0.1', 'meta-llama/Llama-2-7b-chat-hf'],\n",
    "        'temperature_range': (0, 2),\n",
    "        'max_tokens': 2048\n",
    "    },\n",
    "    'cohere': {\n",
    "        'models': ['command', 'command-light', 'command-nightly'],\n",
    "        'temperature_range': (0, 5),\n",
    "        'max_tokens': 4096\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Provider Configuration:\")\n",
    "for provider, config in provider_config.items():\n",
    "    print(f\"\\n{provider.upper()}:\")\n",
    "    print(f\"  Models: {', '.join(config['models'][:2])}...\")\n",
    "    print(f\"  Temp Range: {config['temperature_range']}\")\n",
    "    print(f\"  Max Tokens: {config['max_tokens']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Troubleshooting & Monitoring\n",
    "\n",
    "Debug and monitor LLM interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MCPMonitor:\n",
    "    \"\"\"Monitor and log MCP interactions\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.calls = []\n",
    "    \n",
    "    def log_call(self, provider: str, model: str, prompt_length: int, response_length: int, duration: float):\n",
    "        \"\"\"Log an API call\"\"\"\n",
    "        self.calls.append({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'provider': provider,\n",
    "            'model': model,\n",
    "            'prompt_length': prompt_length,\n",
    "            'response_length': response_length,\n",
    "            'duration': duration\n",
    "        })\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get usage statistics\"\"\"\n",
    "        if not self.calls:\n",
    "            return {}\n",
    "        \n",
    "        return {\n",
    "            'total_calls': len(self.calls),\n",
    "            'providers_used': list(set([c['provider'] for c in self.calls])),\n",
    "            'avg_response_length': sum([c['response_length'] for c in self.calls]) / len(self.calls),\n",
    "            'avg_duration': sum([c['duration'] for c in self.calls]) / len(self.calls)\n",
    "        }\n",
    "\n",
    "monitor = MCPMonitor()\n",
    "print(\"âœ… Monitoring system initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You now have a fully functional MCP-Jupyter Extension with:\n",
    "\n",
    "âœ… **Multi-LLM Support**: OpenAI, Anthropic, Google, Hugging Face, Cohere\n",
    "âœ… **Streaming Responses**: Real-time output from all providers\n",
    "âœ… **Context Management**: Maintain conversation history\n",
    "âœ… **Monitoring**: Track API usage and performance\n",
    "âœ… **Easy Switching**: Change providers with one parameter\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Configure API keys for your preferred providers\n",
    "2. Run the examples in Parts 5-7\n",
    "3. Customize the system instructions as needed\n",
    "4. Build your own applications using the MCPClient\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Repository](https://github.com/soroushbagheri/mcp-jupyter-extension)\n",
    "- [Model Context Protocol](https://modelcontextprotocol.io/)\n",
    "- [Jupyter Documentation](https://jupyter.org/)\n",
    "- [API Documentation Links](https://github.com/soroushbagheri/mcp-jupyter-extension#documentation)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy coding! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}