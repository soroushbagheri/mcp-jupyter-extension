{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhCIL8pfD_oV"
      },
      "source": [
        "# MCP-Jupyter Extension: Multi-LLM Colab Notebook\n",
        "\n",
        "This notebook demonstrates the complete MCP-Jupyter Extension with support for multiple LLMs.\n",
        "\n",
        "**Supported Providers:**\n",
        "- OpenAI (GPT-4, GPT-3.5-Turbo)\n",
        "- Anthropic (Claude 3 family)\n",
        "- Google (Gemini Pro)\n",
        "- Hugging Face (Any HF model)\n",
        "- Cohere (Command models)\n",
        "- Local Models (Ollama, vLLM)\n",
        "\n",
        "**Created:** February 2026\n",
        "**Version:** 1.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OUCGa_FD_ok"
      },
      "source": [
        "## Part 1: Environment Setup and Installation\n",
        "\n",
        "Run this cell first to install all required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tp9bSJYnD_ol",
        "outputId": "d9a1dd37-1727-467e-9bf6-1cea04e928e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Install required packages\n",
        "!pip install --upgrade pip\n",
        "!pip install -q openai anthropic google-generativeai huggingface-hub cohere python-dotenv aiohttp pydantic requests\n",
        "\n",
        "# Verify installations\n",
        "import sys\n",
        "print(f'Python version: {sys.version}')\n",
        "print('All packages installed successfully!')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-26.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-26.0.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-26.0.1\n",
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "All packages installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpnVKS9aD_op"
      },
      "source": [
        "## Part 2: Import Libraries and Configure Credentials\n",
        "\n",
        "Configure your API keys for different LLM providers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VH-BzmezD_op",
        "outputId": "2cad062c-d672-410e-8598-8c153f5adf83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import json\n",
        "from typing import Optional, Dict, List, Iterator\n",
        "import asyncio\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Import LLM libraries\n",
        "import openai\n",
        "from anthropic import Anthropic\n",
        "import google.generativeai as genai\n",
        "from huggingface_hub import InferenceClient\n",
        "import cohere\n",
        "\n",
        "print('âœ… All libraries imported successfully!')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py:55: FutureWarning: \n",
            "\n",
            "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
            "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
            "See README for more details:\n",
            "\n",
            "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
            "\n",
            "  loader.exec_module(module)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzV1AATnD_oq"
      },
      "source": [
        "### Configure API Keys\n",
        "\n",
        "Enter your API keys below. You can get them from:\n",
        "- [OpenAI](https://platform.openai.com/api-keys)\n",
        "- [Anthropic](https://console.anthropic.com/)\n",
        "- [Google AI](https://ai.google.dev/)\n",
        "- [Hugging Face](https://huggingface.co/settings/tokens)\n",
        "- [Cohere](https://dashboard.cohere.ai/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGzkTrs9D_or"
      },
      "source": [
        "from getpass import getpass\n",
        "\n",
        "# Configure API keys (use getpass for security)\n",
        "api_keys = {}\n",
        "\n",
        "# OpenAI\n",
        "openai_key = getpass('OpenAI API Key (or press Enter to skip): ')\n",
        "if openai_key:\n",
        "    os.environ['OPENAI_API_KEY'] = openai_key\n",
        "    api_keys['openai'] = openai_key\n",
        "\n",
        "# Anthropic\n",
        "#anthropic_key = getpass('Anthropic API Key (or press Enter to skip): ')\n",
        "#if anthropic_key:\n",
        "    #os.environ['ANTHROPIC_API_KEY'] = anthropic_key\n",
        "    #api_keys['anthropic'] = anthropic_key\n",
        "\n",
        "# Google Gemini\n",
        "#google_key = getpass('Google API Key (or press Enter to skip): ')\n",
        "#if google_key:\n",
        " #   os.environ['GOOGLE_API_KEY'] = google_key\n",
        "  #  api_keys['google'] = google_key\n",
        "   # genai.configure(api_key=google_key)\n",
        "\n",
        "# Hugging Face\n",
        "hf_key = getpass('Hugging Face API Key (or press Enter to skip): ')\n",
        "if hf_key:\n",
        "    os.environ['HF_API_KEY'] = hf_key\n",
        "    api_keys['huggingface'] = hf_key\n",
        "\n",
        "# Cohere\n",
        "cohere_key = getpass('Cohere API Key (or press Enter to skip): ')\n",
        "if cohere_key:\n",
        "    os.environ['COHERE_API_KEY'] = cohere_key\n",
        "    api_keys['cohere'] = cohere_key\n",
        "\n",
        "print(f'\\nâœ… Configured {len(api_keys)} API providers')\n",
        "print(f'Available providers: {list(api_keys.keys())}')"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60sUImFAD_ou"
      },
      "source": [
        "## Part 3: Define MCP Client Classes\n",
        "\n",
        "Core classes for managing LLM interactions through the MCP protocol."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUNt-6yND_ox"
      },
      "source": [
        "@dataclass\n",
        "class MCPResponse:\n",
        "    \"\"\"Response object from LLM\"\"\"\n",
        "    content: str\n",
        "    provider: str\n",
        "    model: str\n",
        "    timestamp: str\n",
        "    tokens_used: Optional[int] = None\n",
        "    finish_reason: Optional[str] = None\n",
        "\n",
        "\n",
        "class BaseLLMProvider(ABC):\n",
        "    \"\"\"Base class for LLM providers\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, model: str):\n",
        "        self.api_key = api_key\n",
        "        self.model = model\n",
        "\n",
        "    @abstractmethod\n",
        "    def query(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> MCPResponse:\n",
        "        \"\"\"Send a query to the LLM\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def stream(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> Iterator[str]:\n",
        "        \"\"\"Stream response from the LLM\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class OpenAIProvider(BaseLLMProvider):\n",
        "    \"\"\"OpenAI GPT provider\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, model: str = \"gpt-4\"):\n",
        "        super().__init__(api_key, model)\n",
        "        self.client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "    def query(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> MCPResponse:\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens\n",
        "        )\n",
        "        return MCPResponse(\n",
        "            content=response.choices[0].message.content,\n",
        "            provider=\"OpenAI\",\n",
        "            model=self.model,\n",
        "            timestamp=datetime.now().isoformat(),\n",
        "            tokens_used=response.usage.total_tokens,\n",
        "            finish_reason=response.choices[0].finish_reason\n",
        "        )\n",
        "\n",
        "    def stream(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> Iterator[str]:\n",
        "        stream = self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "            stream=True\n",
        "        )\n",
        "        for chunk in stream:\n",
        "            if chunk.choices[0].delta.content:\n",
        "                yield chunk.choices[0].delta.content\n",
        "\n",
        "\n",
        "class AnthropicProvider(BaseLLMProvider):\n",
        "    \"\"\"Anthropic Claude provider\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, model: str = \"claude-3-sonnet-20240229\"):\n",
        "        super().__init__(api_key, model)\n",
        "        self.client = Anthropic(api_key=api_key)\n",
        "\n",
        "    def query(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> MCPResponse:\n",
        "        response = self.client.messages.create(\n",
        "            model=self.model,\n",
        "            max_tokens=max_tokens,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=temperature\n",
        "        )\n",
        "        return MCPResponse(\n",
        "            content=response.content[0].text,\n",
        "            provider=\"Anthropic\",\n",
        "            model=self.model,\n",
        "            timestamp=datetime.now().isoformat(),\n",
        "            tokens_used=response.usage.output_tokens + response.usage.input_tokens,\n",
        "            finish_reason=response.stop_reason\n",
        "        )\n",
        "\n",
        "    def stream(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> Iterator[str]:\n",
        "        with self.client.messages.stream(\n",
        "            model=self.model,\n",
        "            max_tokens=max_tokens,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=temperature\n",
        "        ) as stream:\n",
        "            for text in stream.text_stream:\n",
        "                yield text\n",
        "\n",
        "\n",
        "class GoogleProvider(BaseLLMProvider):\n",
        "    \"\"\"Google Gemini provider\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, model: str = \"gemini-pro\"):\n",
        "        super().__init__(api_key, model)\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.client = genai.GenerativeModel(model_name=model)\n",
        "\n",
        "    def query(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> MCPResponse:\n",
        "        response = self.client.generate_content(\n",
        "            prompt,\n",
        "            generation_config=genai.types.GenerationConfig(\n",
        "                max_output_tokens=max_tokens,\n",
        "                temperature=temperature\n",
        "            )\n",
        "        )\n",
        "        return MCPResponse(\n",
        "            content=response.text,\n",
        "            provider=\"Google\",\n",
        "            model=self.model,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "    def stream(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> Iterator[str]:\n",
        "        response = self.client.generate_content(\n",
        "            prompt,\n",
        "            generation_config=genai.types.GenerationConfig(\n",
        "                max_output_tokens=max_tokens,\n",
        "                temperature=temperature\n",
        "            ),\n",
        "            stream=True\n",
        "        )\n",
        "        for chunk in response:\n",
        "            if chunk.text:\n",
        "                yield chunk.text\n",
        "\n",
        "\n",
        "class HuggingFaceProvider(BaseLLMProvider):\n",
        "    \"\"\"Hugging Face Inference API provider\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, model: str = \"mistralai/Mistral-7B-Instruct-v0.1\"):\n",
        "        super().__init__(api_key, model)\n",
        "        self.client = InferenceClient(api_key=api_key)\n",
        "\n",
        "    def query(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> MCPResponse:\n",
        "        response = self.client.text_generation(\n",
        "            prompt=prompt,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=temperature\n",
        "        )\n",
        "        return MCPResponse(\n",
        "            content=response,\n",
        "            provider=\"HuggingFace\",\n",
        "            model=self.model,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "    def stream(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> Iterator[str]:\n",
        "        for token in self.client.text_generation(\n",
        "            prompt=prompt,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            stream=True\n",
        "        ):\n",
        "            yield token\n",
        "\n",
        "\n",
        "class CohereProvider(BaseLLMProvider):\n",
        "    \"\"\"Cohere API provider\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, model: str = \"command\"):\n",
        "        super().__init__(api_key, model)\n",
        "        self.client = cohere.Client(api_key=api_key)\n",
        "\n",
        "    def query(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> MCPResponse:\n",
        "        response = self.client.generate(\n",
        "            prompt=prompt,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature\n",
        "        )\n",
        "        return MCPResponse(\n",
        "            content=response.generations[0].text,\n",
        "            provider=\"Cohere\",\n",
        "            model=self.model,\n",
        "            timestamp=datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "    def stream(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> Iterator[str]:\n",
        "        # Note: Cohere streaming not directly supported, fall back to regular generation\n",
        "        response = self.query(prompt, temperature, max_tokens)\n",
        "        yield response.content\n",
        "\n",
        "\n",
        "print('âœ… Provider classes defined successfully!')"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loX9sBovD_oz"
      },
      "source": [
        "## Part 4: Define MCPClient for Easy Access\n",
        "\n",
        "High-level client to switch between providers seamlessly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MlRfNhAD_o0"
      },
      "source": [
        "class MCPClient:\n",
        "    \"\"\"High-level client for MCP-Jupyter Extension\"\"\"\n",
        "\n",
        "    PROVIDERS = {\n",
        "        'openai': OpenAIProvider,\n",
        "        'anthropic': AnthropicProvider,\n",
        "        'google': GoogleProvider,\n",
        "        'huggingface': HuggingFaceProvider,\n",
        "        'cohere': CohereProvider,\n",
        "    }\n",
        "\n",
        "    def __init__(self, provider: str, model: str = None):\n",
        "        if provider not in self.PROVIDERS:\n",
        "            raise ValueError(f\"Provider '{provider}' not supported. Choose from {list(self.PROVIDERS.keys())}\")\n",
        "\n",
        "        self.provider_name = provider\n",
        "        api_key = os.environ.get(f\"{provider.upper()}_API_KEY\".replace('OPENAI_API_KEY', 'OPENAI_API_KEY').replace('ANTHROPIC', 'ANTHROPIC').replace('GOOGLE', 'GOOGLE').replace('HUGGINGFACE', 'HF').replace('COHERE', 'COHERE'))\n",
        "\n",
        "        if provider == 'openai':\n",
        "            api_key = os.environ.get('OPENAI_API_KEY')\n",
        "        elif provider == 'anthropic':\n",
        "            api_key = os.environ.get('ANTHROPIC_API_KEY')\n",
        "        elif provider == 'google':\n",
        "            api_key = os.environ.get('GOOGLE_API_KEY')\n",
        "        elif provider == 'huggingface':\n",
        "            api_key = os.environ.get('HF_API_KEY')\n",
        "        elif provider == 'cohere':\n",
        "            api_key = os.environ.get('COHERE_API_KEY')\n",
        "\n",
        "        if not api_key:\n",
        "            raise ValueError(f\"API key not found for provider '{provider}'.\")\n",
        "\n",
        "        # Default models\n",
        "        default_models = {\n",
        "            'openai': 'gpt-4',\n",
        "            'anthropic': 'claude-3-sonnet-20240229',\n",
        "            'google': 'gemini-pro',\n",
        "            'huggingface': 'mistralai/Mistral-7B-Instruct-v0.1',\n",
        "            'cohere': 'command'\n",
        "        }\n",
        "\n",
        "        model = model or default_models[provider]\n",
        "        self.provider = self.PROVIDERS[provider](api_key, model)\n",
        "\n",
        "    def query(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000) -> MCPResponse:\n",
        "        \"\"\"Send a query and get response\"\"\"\n",
        "        try:\n",
        "            return self.provider.query(prompt, temperature, max_tokens)\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error with {self.provider_name}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def stream(self, prompt: str, temperature: float = 0.7, max_tokens: int = 1000):\n",
        "        \"\"\"Stream response from LLM\"\"\"\n",
        "        try:\n",
        "            for chunk in self.provider.stream(prompt, temperature, max_tokens):\n",
        "                print(chunk, end='', flush=True)\n",
        "                yield chunk\n",
        "        except Exception as e:\n",
        "            print(f\"\\nâŒ Error streaming from {self.provider_name}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "print('âœ… MCPClient defined successfully!')"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_9ZYdRHD_o1"
      },
      "source": [
        "## Part 5: Example 1 - Single Provider Query\n",
        "\n",
        "Test with a single LLM provider."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzeYw10hD_o3"
      },
      "source": [
        "# Example query\n",
        "test_prompt = \"Explain quantum computing in 3 sentences.\"\n",
        "\n",
        "# Choose a provider that you configured\n",
        "available_providers = list(api_keys.keys())\n",
        "\n",
        "if available_providers:\n",
        "    provider = available_providers[0]\n",
        "    print(f\"ðŸš€ Testing with {provider}...\\n\")\n",
        "\n",
        "    try:\n",
        "        client = MCPClient(provider=provider)\n",
        "        response = client.query(test_prompt)\n",
        "\n",
        "        print(f\"Provider: {response.provider}\")\n",
        "        print(f\"Model: {response.model}\")\n",
        "        print(f\"Response:\\n{response.content}\")\n",
        "        print(f\"\\nTimestamp: {response.timestamp}\")\n",
        "        if response.tokens_used:\n",
        "            print(f\"Tokens used: {response.tokens_used}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "else:\n",
        "    print(\"No API keys configured. Please run Part 2 to add API keys.\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABvE-1B3D_o3"
      },
      "source": [
        "## Part 6: Example 2 - Streaming Response\n",
        "\n",
        "Get real-time streamed responses from LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyHcaq2tD_o4"
      },
      "source": [
        "# Streaming example\n",
        "stream_prompt = \"Write a creative haiku about artificial intelligence.\"\n",
        "\n",
        "if available_providers:\n",
        "    provider = available_providers[0]\n",
        "    print(f\"ðŸŒŠ Streaming from {provider}...\\n\")\n",
        "\n",
        "    try:\n",
        "        client = MCPClient(provider=provider)\n",
        "        print(\"Response: \")\n",
        "        for _ in client.stream(stream_prompt):\n",
        "            pass\n",
        "        print(\"\\n\\nâœ… Streaming complete!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lpZJvHsD_o5"
      },
      "source": [
        "## Part 7: Example 3 - Multi-Provider Comparison\n",
        "\n",
        "Compare responses from multiple LLM providers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTuWMItXD_o6"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Multi-provider comparison\n",
        "comparison_prompt = \"What are the top 3 applications of AI in healthcare?\"\n",
        "\n",
        "results = []\n",
        "\n",
        "for provider in available_providers:\n",
        "    print(f\"ðŸ“ Querying {provider}...\")\n",
        "    try:\n",
        "        client = MCPClient(provider=provider)\n",
        "        response = client.query(comparison_prompt, max_tokens=300)\n",
        "        results.append({\n",
        "            'Provider': response.provider,\n",
        "            'Model': response.model,\n",
        "            'Response': response.content[:200] + '...' if len(response.content) > 200 else response.content,\n",
        "            'Timestamp': response.timestamp\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error with {provider}: {e}\")\n",
        "\n",
        "if results:\n",
        "    df = pd.DataFrame(results)\n",
        "    print(\"\\nðŸ“Š Comparison Results:\")\n",
        "    display(df)\n",
        "else:\n",
        "    print(\"No results to compare.\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SRL_7YqD_o7"
      },
      "source": [
        "## Part 8: Context Management\n",
        "\n",
        "Manage conversation context and system instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnt5yKCOD_o8"
      },
      "source": [
        "class ContextManager:\n",
        "    \"\"\"Manages conversation context and system instructions\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.context = []\n",
        "        self.system_instruction = \"You are a helpful AI assistant.\"\n",
        "\n",
        "    def add_message(self, role: str, content: str):\n",
        "        \"\"\"Add a message to context\"\"\"\n",
        "        self.context.append({\"role\": role, \"content\": content})\n",
        "\n",
        "    def add_system_instruction(self, instruction: str):\n",
        "        \"\"\"Set system instruction\"\"\"\n",
        "        self.system_instruction = instruction\n",
        "\n",
        "    def clear(self):\n",
        "        \"\"\"Clear all context\"\"\"\n",
        "        self.context = []\n",
        "\n",
        "    def get_context(self) -> Dict:\n",
        "        \"\"\"Get full context\"\"\"\n",
        "        return {\n",
        "            \"system\": self.system_instruction,\n",
        "            \"messages\": self.context\n",
        "        }\n",
        "\n",
        "\n",
        "# Example usage\n",
        "ctx = ContextManager()\n",
        "ctx.add_system_instruction(\"You are an expert Python programmer.\")\n",
        "ctx.add_message(\"user\", \"What's the best way to handle errors in Python?\")\n",
        "\n",
        "print(\"Context:\")\n",
        "print(json.dumps(ctx.get_context(), indent=2))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIMEv1ayD_o-"
      },
      "source": [
        "## Part 9: Advanced Configuration\n",
        "\n",
        "Configure provider-specific settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QK5Y_CquD_o-"
      },
      "source": [
        "# Configuration dictionary\n",
        "provider_config = {\n",
        "    'openai': {\n",
        "        'models': ['gpt-4', 'gpt-4-turbo-preview', 'gpt-3.5-turbo'],\n",
        "        'temperature_range': (0, 2),\n",
        "        'max_tokens': 4096\n",
        "    },\n",
        "    'anthropic': {\n",
        "        'models': ['claude-3-opus-20240229', 'claude-3-sonnet-20240229', 'claude-3-haiku-20240307'],\n",
        "        'temperature_range': (0, 1),\n",
        "        'max_tokens': 4096\n",
        "    },\n",
        "    'google': {\n",
        "        'models': ['gemini-pro', 'gemini-pro-vision'],\n",
        "        'temperature_range': (0, 2),\n",
        "        'max_tokens': 32000\n",
        "    },\n",
        "    'huggingface': {\n",
        "        'models': ['mistralai/Mistral-7B-Instruct-v0.1', 'meta-llama/Llama-2-7b-chat-hf'],\n",
        "        'temperature_range': (0, 2),\n",
        "        'max_tokens': 2048\n",
        "    },\n",
        "    'cohere': {\n",
        "        'models': ['command', 'command-light', 'command-nightly'],\n",
        "        'temperature_range': (0, 5),\n",
        "        'max_tokens': 4096\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Provider Configuration:\")\n",
        "for provider, config in provider_config.items():\n",
        "    print(f\"\\n{provider.upper()}:\")\n",
        "    print(f\"  Models: {', '.join(config['models'][:2])}...\")\n",
        "    print(f\"  Temp Range: {config['temperature_range']}\")\n",
        "    print(f\"  Max Tokens: {config['max_tokens']}\")\n"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGKT7iDBD_pA"
      },
      "source": [
        "## Part 10: Troubleshooting & Monitoring\n",
        "\n",
        "Debug and monitor LLM interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiKRj_ITD_pA"
      },
      "source": [
        "class MCPMonitor:\n",
        "    \"\"\"Monitor and log MCP interactions\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.calls = []\n",
        "\n",
        "    def log_call(self, provider: str, model: str, prompt_length: int, response_length: int, duration: float):\n",
        "        \"\"\"Log an API call\"\"\"\n",
        "        self.calls.append({\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'provider': provider,\n",
        "            'model': model,\n",
        "            'prompt_length': prompt_length,\n",
        "            'response_length': response_length,\n",
        "            'duration': duration\n",
        "        })\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Get usage statistics\"\"\"\n",
        "        if not self.calls:\n",
        "            return {}\n",
        "\n",
        "        return {\n",
        "            'total_calls': len(self.calls),\n",
        "            'providers_used': list(set([c['provider'] for c in self.calls])),\n",
        "            'avg_response_length': sum([c['response_length'] for c in self.calls]) / len(self.calls),\n",
        "            'avg_duration': sum([c['duration'] for c in self.calls]) / len(self.calls)\n",
        "        }\n",
        "\n",
        "monitor = MCPMonitor()\n",
        "print(\"âœ… Monitoring system initialized!\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP8U6KqcD_pC"
      },
      "source": [
        "## Summary\n",
        "\n",
        "You now have a fully functional MCP-Jupyter Extension with:\n",
        "\n",
        "âœ… **Multi-LLM Support**: OpenAI, Anthropic, Google, Hugging Face, Cohere\n",
        "âœ… **Streaming Responses**: Real-time output from all providers\n",
        "âœ… **Context Management**: Maintain conversation history\n",
        "âœ… **Monitoring**: Track API usage and performance\n",
        "âœ… **Easy Switching**: Change providers with one parameter\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Configure API keys for preferred providers\n",
        "2. Run the examples in Parts 5-7\n",
        "3. Customize the system instructions as needed\n",
        "4. Build applications using the MCPClient\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [Repository](https://github.com/soroushbagheri/mcp-jupyter-extension)\n",
        "- [Model Context Protocol](https://modelcontextprotocol.io/)\n",
        "- [Jupyter Documentation](https://jupyter.org/)\n",
        "- [API Documentation Links](https://github.com/soroushbagheri/mcp-jupyter-extension#documentation)\n",
        "\n",
        "---\n",
        "\n",
        "**Happy coding! ðŸš€**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}